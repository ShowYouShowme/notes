# 第一章 绪论

本文主要收集笔者多年游戏开发中常用的服务器的架构。

除了connector 和client保持长连接，上游服务之间不要建立长连接，断线重连的问题会变得非常麻烦。



服务设计

1. 每个服务有个配置文件example.conf；开发时copy一份命名为rummy.conf或者dev.conf(不提交git 仓库)，然后可以自己修改;线上部署时，从example.conf copy一份命名为prod.conf,修改；这样可以不暴露数据库信息到git 代码仓库里面！该方案比 弄三个配置文件 dev.conf、stage.conf、prod.conf 要好很多




# 第二章 基于消息队列的服务架构

用户登录，发送token给gateway，此时gateway利用grpc访问loginservice校验，不要用http，http请求代码太复杂!



mq使用场景：gateway收到数据后转发到上游服务，上游服务给client推送消息

其它场景用grpc：比如上游服务之间互相调用、统计（事件发送到mq，由统计服务处理的方案不如grpc方便，抛弃掉！）



## 2.1 采用消息队列Rabbitmq

```
connector(单节点)   ------>  Rabbitmq  --------->  Game

goldService --> 管理用户金币等资源,全部数据库操作都在此服务,其它服务需要操作db时grpc访问它

configservice ---> 管理配置文件,产品通过管理后台修改配置文件,其它服务通过grpc 请求该服务获取配置
```

1. Rabbitmq有两个队列，一个队列存放client的请求，一个队列存放上游服务推送给client的数据

2. connector的数据结构

   ```go
   const (
   	EventConnect = 1
   	EventMessage = 2
   	EventClose   = 3
   	EventError   = 4
   )
   
   // onConnect
   type ConnectMsg struct {
   	Conn *websocket.Conn
   	Addr string
   	Uid  int
   }
   
   // onMessage
   type MessageMsg struct {
   	Addr string
   	Data []byte
   }
   
   // OnClose 和 OnError 时, 仅传入addr
   ```

3. Game 模拟skynet的做法，把全部的服务写到 一个进程里面，每个服务使用一个线程。

4. 服务规划

   ```ini
   ; 开发人员不足的时候，可以将login、api-grpc、widget合并到一起,
   ; 该服务有三个文件夹login、api-grpc和widget；widget下面有根据不同的
   ; mq消息有不同的文件夹。可以根据参数决定启动哪个服务;也可以全部封装到一个进程里
   [gateway]
   type = "server"
   desc = "网关,用户处理客户端websocket链接和推送消息; 只有一个实例"
   
   [login]
   type = "server"
   desc  = "登录服务, client请求获取token、token校验、推送UserInfo、热更新版本对比"
   desc2 = "login会提供http接口、并且处理rabbitmq的消息"
   
   [admin]
   type = "server"
   desc  = "管理后台采用前后端分离架构，比如gin-vue-admin"
   desc2 = "管理后台项目里只有管理后台相关代码,不要和业务代码混到一起"
   
   [api-grpc]
   type = "server"
   desc = "各种rpc的接口封装到一起;比如修改金币;该服务会设计大量DB的操作"
   
   [game]
   type = "server"
   desc = "游戏服务，有状态;需要操作用户金币时，rpc请求Grpc服务"
   
   [widget]
   type = "server"
   desc = "各种活动; 改服务开启多个线程；每个线程消费一个类型的mq消息; 每个服务有自己的comsumer 和 publisher"
   desc2 = "consumer 和 pulisher线程不安全，不要共享" 
   
   [common]
   type = "proto"
   desc = “网络消息proto、grpc的proto、错误码的proto；作为其它项目的子模块”
   
   [models]
   type = "package"
   desc = "封装为一个单独的go package；将全部数据库模型的定义,处理函数封装在里面"
   desc2 = "login、admin、api-grpc和 widget都需要用到"
   
   [中间件]
   数据库    = "MySQL || Mongo"
   消息队列   = "RabbitMQ[解耦]"
   配置中心   = "Redis[管理全部服务配置,实时更新配置]"
   缓存      = "Redis[可以考虑去掉,因为会引入新问题(一致性)]"
   ```

   





## 2.2 采用Redis做消息队列 Pub/sub

Redis做消息队列比RabbitMQ代码简单得多[非常推荐这种架构]

1. 使用list作队列(消息不会丢失)：一个消息不能被多个消费者消费；没有ack机制，消费过程中上游服务宕机导致消息丢失（对游戏来讲这两个都不是问题）

   ```go
   package main
   
   import (
   	"context"
   	"fmt"
   	redis "github.com/redis/go-redis/v9"
   )
   var ctx = context.Background()
   func main() {
   	rdb := redis.NewClient(&redis.Options{
   		Addr:     "localhost:6379",
   		Password: "", // no password set
   		DB:       0,  // use default DB
   	})
   
   	rdb.LPush(ctx, "routing_exchange", []byte{0x01, 0x02, 0x03})
   
   	// 0 表示永不超时
   	result, err := rdb.BRPop(ctx, 0, "routing_exchange").Result()
   	if err != nil {
   		panic(err)
   	}
       // result[0] 是 key
   	// result[1] 是 value
   	fmt.Printf(result[0], result[1])
   }
   ```

2. 采用Pub/Sub 发布消息时，没有订阅者会丢失(对游戏来正好合适)；当client堆积太多消息，占用太多内存时会被redis断开（可以更改配置解决）；记得先启动上游服务再启动gateway

   ```go
   func testPubSub() {
   	rdb := redis.NewClient(&redis.Options{
   		Addr:     "localhost:6379",
   		Password: "", // no password set
   		DB:       0,  // use default DB
   	})
   
   	// 订阅
   	sub := rdb.Subscribe(ctx, "channel")
   	ch := sub.Channel()
   	for msg := range ch {
   		fmt.Printf(msg.Channel, msg.Payload)
   	}
   
   	// 发布
   	err := rdb.Publish(ctx, "channel", "helloWorld").Err()
   	if err != nil {
   		panic(err)
   	}
   }
   ```

   相关配置

   ```shell
   client-output-buffer-limit pubsub 32mb 8mb 60
   #解释
   32mb：缓冲区一旦超过 32MB，Redis 直接强制把消费者踢下线
   8mb + 60：缓冲区超过 8MB，并且持续 60 秒，Redis 也会把消费者踢下线
   
   # 永远不踢消费者下线
   client-output-buffer-limit pubsub 0 0 0
   ```

   



## 2.2 采用zeromq

zmq的好处是不需要处理连接的重连、不用管服务启动次序等；缺点是go语言支持不好，安装依赖非常麻烦！

1. 推荐架构：connector 是发布者，上游服务是订阅者。connector收到client请求时，发布消息，上游服务过滤后处理！上游服务器需要推送消息给client时，采用push--pull模式（connector是单节点）。

   ```
                                --------------> login(subscribe login)
   
   connector --publish-->       ---------------> game(subscribe game)
   
                                ----------------> store(subscribe store)
   
                                -----------------> task(subscribe task)
                                
                                
   # 推送消息时
   
   connector  <---(pull)--          <-----push-----   game
   ```

   

2. 架构一：connector收到client数据用PIPE模式 push给 Game； 每个connector都订阅Game的消息，当Game需要主动推送消息给client，利用ZMQ.PUB

   ```
   connector1 PUSH ----->
   
   connector2 PUSH -----> PULL  GAME
   
   connector3 PUSH ----->
   ```

3. 架构二：和架构一相比，有一个专门的push 服务；每个connector多监听一个ZMQ.REP 端口；启动时将地址（sid， url）注册到push服务；push服务收到推送数据请求时，根据sid 发起 ZMQ.REQ 发起请求

   ```
   
                                   PUSH-SERVER
   
   connector1 PUSH ----->
   
   connector2 PUSH -----> PULL  GAME
   
   connector3 PUSH ----->
   ```

   

4. 架构三：采用zeromq，只能单个connector [最简单]

   ```shell
   connector PUSH -----> PULL  GAME
         
             PULL <----- PUSH  GAME 
   ```

   





## 2.4 服务划分为有状态和无状态

```

client ---- http ------>  无状态服务

                   connector1
                                 ws/zmq
client   ---->     connector2   -------->  有状态服务,比如玩法
               
                   connector3
```









# 第三章 基于GRPC的协议

首选这种架构

1. gateway定义接口

   + sendToPlayer(uid, cmd, bytes)：上游服务通过此接口推送消息

   + register：上游服务将自己的地址注册到gateway，然后gateway就可以实现发布订阅的功能了

2. 每个上游服务都有一个接口deliver(uid, cmd, bytes), gateway 通过此接口将对应数据包转发给服务

3. 每个上游服务有个app类(单例)，里面有两函数（OnMessage、schedule），deliver调用OnMessage将消息发送到app的channel，channel带有缓冲区（1024或者更大）；schedule里面不断从channel里获取消息来处理

4. 服务架构

5. 使用这种架构上游服务不需要打包、解包Packet，只需要打包、解包具体的业务消息

   ```
                        pub/sub  game.OnMessage(uid, cmd, bytes)
   connector[单节点]  GRPC---------> game
                       
                       sendToPlayer()
   					<-----------
   ```

   




# 第四章 基于http的协议

这种解决方案的有点在于简单



1. 客户端通过http协议请求服务器数据，服务端有一个专门的推送服务，每个客户端和推送服务建立长连接
2. 客户端和服务器采用json格式通信，服务端推送给客户端的数据也是json
3. token的时间可以设置为1个月或者永远







# 第五章 连接服务的设计



## 5.1 采用云风的思路

云风的连接器没有业务，会更加通用；但是不适用于websocket的架构！

url = https://blog.codingnow.com/2006/04/iocp_kqueue_epoll.html



## 5.2 传统思路

传统思路相对云风的，会更加简洁。

1. client 访问login 返回token
2. connector 收到client登录请求时，rpc请求登录服务返回uid，直接和socket 进行绑定





# 第六章 推送服务的设计

推送服务与连接服务合二为一！跑马灯、公告之类的需求，需要向部分/全部玩家推送数据，gateway具备推送的功能，管理用户链接和推送信息！

1. client 拿token 连接gateway时，gateway会请求登录服校验token返回uid，绑定session，最后再rpc请求登录服推送用户信息给client（注意时序）。

2. 其它请求根据Cmd的前三位发送到对应的rabbitmq队列，由上游服务处理，利用rabbitmq可以实现解耦！

3. 另一种设计方案：client 拿token 连接gateway时，gateway往mq 里丢一个消息(带上token和wsAddr)；上游的登录服消费消息后推送两个消息给gateway，一个是（uid和wsAddr，gateway用来绑定session），另一个是推送给client的userInfo。这样gateway可以和登录服解耦，gateway仅依赖于rabbitmq！ A服务 调用B服务的接口，称为A服务依赖B服务！



# 第六章  财务服务的设计

全部数据库相关的操作，全部封装到财务服务，做成无状态的



# 第七章 登录服务的设计

1. 协议采用http
2. 做成无状态的



# 第八章 配置中心

1. 产品配置的数值用grpc从数据库获取，并且订阅redis的 configUpdate channel,收到消息后再次调用grpc获取新配置
2. 服务配置，比如数据库地址、数据库账号密码、grpc的地址等配置在json文件里面，准备example.conf[提交git]，部署时copy 为 rummy.conf[不提交] 然后启动时-conf=rummy.conf启动

配置中心用来管理全部微服务的配置[产品配置的数值]，并且能够实时更新服务配置！

1. 使用zookeeper作为配置中心，将常用服务名--地址信息注册进去（其它配置可以根据实际情况处理）。每个服务启动时 命令行指定 zk的地址即可。
2. 配置中心也可以使用redis，当需要实时更新时，往redis的channl里面发消息，业务服收到消息主动去redis拉取新的配置。虽然zk也能有相同的功能，不过建议使用redis。redis的包更加稳定。尽量不要在项目里引入过多的中间件！go 语言推荐使用redis 作为配置中心，因为zookeeper的客户端官方并没有golang版本的！



# 第九章 仓库管理

服务端代码就两个仓库

1. 管理后台admin
2. rummy：rummy下面有多个文件夹，每个文件夹是一个golang项目，代表一个服务；每个服务都有一个makefile文件。
   + login：登录服
   + gateway：网关
   + api-grpc：各种资源操作grpc接口
   + game：游戏服务
   + widget：各个功能模块
   + common：proto文件定义
   + models：数据库定义



# 第十章 服务启动次序

rummy服务启动时，需要grpc 请求配置数据，这时候启动就得先启动配置服务。设计方案：启动时for 循环读取配置文件，如果失败了，等5s再读取，直到成功为止。这种方式可以解决服务启动次序问题。