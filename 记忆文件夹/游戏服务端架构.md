# 第一章 绪论

本文主要收集笔者多年游戏开发中常用的服务器的架构。

除了connector 和client保持长连接，上游服务之间不要建立长连接，断线重连的问题会变得非常麻烦。



# 第二章 基于消息队列的服务架构

用户登录，发送token给gateway，此时gateway利用grpc访问loginservice校验，不要用http，http请求代码太复杂!



## 2.1 采用消息队列Rabbitmq

```
connector(单节点)   ------>  Rabbitmq  --------->  Game

goldService --> 管理用户金币等资源,全部数据库操作都在此服务,其它服务需要操作db时grpc访问它

configservice ---> 管理配置文件,产品通过管理后台修改配置文件,其它服务通过grpc 请求该服务获取配置
```

1. Rabbitmq有两个队列，一个队列存放client的请求，一个队列存放上游服务推送给client的数据

2. connector的数据结构

   ```go
   const (
   	EventConnect = 1
   	EventMessage = 2
   	EventClose   = 3
   	EventError   = 4
   )
   
   // onConnect
   type ConnectMsg struct {
   	Conn *websocket.Conn
   	Addr string
   	Uid  int
   }
   
   // onMessage
   type MessageMsg struct {
   	Addr string
   	Data []byte
   }
   
   // OnClose 和 OnError 时, 仅传入addr
   ```

3. Game 模拟skynet的做法，把全部的服务写到 一个进程里面，每个服务使用一个线程。





## 2.2 采用Redis做消息队列 Pub/sub

Redis做消息队列比RabbitMQ代码简单得多[非常推荐这种架构]

1. 使用list作队列(消息不会丢失)：一个消息不能被多个消费者消费；没有ack机制，消费过程中上游服务宕机导致消息丢失（对游戏来讲这两个都不是问题）

   ```go
   package main
   
   import (
   	"context"
   	"fmt"
   	redis "github.com/redis/go-redis/v9"
   )
   var ctx = context.Background()
   func main() {
   	rdb := redis.NewClient(&redis.Options{
   		Addr:     "localhost:6379",
   		Password: "", // no password set
   		DB:       0,  // use default DB
   	})
   
   	rdb.LPush(ctx, "routing_exchange", []byte{0x01, 0x02, 0x03})
   
   	// 0 表示永不超时
   	result, err := rdb.BRPop(ctx, 0, "routing_exchange").Result()
   	if err != nil {
   		panic(err)
   	}
       // result[0] 是 key
   	// result[1] 是 value
   	fmt.Printf(result[0], result[1])
   }
   ```

2. 采用Pub/Sub 发布消息时，没有订阅者会丢失(对游戏来正好合适)；当client堆积太多消息，占用太多内存时会被redis断开（可以更改配置解决）；记得先启动上游服务再启动gateway

   ```go
   func testPubSub() {
   	rdb := redis.NewClient(&redis.Options{
   		Addr:     "localhost:6379",
   		Password: "", // no password set
   		DB:       0,  // use default DB
   	})
   
   	// 订阅
   	sub := rdb.Subscribe(ctx, "channel")
   	ch := sub.Channel()
   	for msg := range ch {
   		fmt.Printf(msg.Channel, msg.Payload)
   	}
   
   	// 发布
   	err := rdb.Publish(ctx, "channel", "helloWorld").Err()
   	if err != nil {
   		panic(err)
   	}
   }
   ```

   相关配置

   ```shell
   client-output-buffer-limit pubsub 32mb 8mb 60
   #解释
   32mb：缓冲区一旦超过 32MB，Redis 直接强制把消费者踢下线
   8mb + 60：缓冲区超过 8MB，并且持续 60 秒，Redis 也会把消费者踢下线
   
   # 永远不踢消费者下线
   client-output-buffer-limit pubsub 0 0 0
   ```

   



## 2.2 采用zeromq

zmq的好处是不需要处理连接的重连、不用管服务启动次序等；缺点是go语言支持不好，安装依赖非常麻烦！

1. 架构一：connector收到client数据用PIPE模式 push给 Game； 每个connector都订阅Game的消息，当Game需要主动推送消息给client，利用ZMQ.PUB

   ```
   connector1 PUSH ----->
   
   connector2 PUSH -----> PULL  GAME
   
   connector3 PUSH ----->
   ```

2. 架构二：和架构一相比，有一个专门的push 服务；每个connector多监听一个ZMQ.REP 端口；启动时将地址（sid， url）注册到push服务；push服务收到推送数据请求时，根据sid 发起 ZMQ.REQ 发起请求

   ```
   
                                   PUSH-SERVER
   
   connector1 PUSH ----->
   
   connector2 PUSH -----> PULL  GAME
   
   connector3 PUSH ----->
   ```

   

3. 架构三：采用zeromq，只能单个connector [最简单]

   ```shell
   connector PUSH -----> PULL  GAME
         
             PULL <----- PUSH  GAME 
   ```

   





## 2.4 服务划分为有状态和无状态

```

client ---- http ------>  无状态服务

                   connector1
                                 ws/zmq
client   ---->     connector2   -------->  有状态服务,比如玩法
               
                   connector3
```









# 第三章 基于GRPC的协议

首选这种架构

1. gateway定义接口

   + sendToPlayer(uid, cmd, bytes)：上游服务通过此接口推送消息

   + register：上游服务将自己的地址注册到gateway，然后gateway就可以实现发布订阅的功能了

2. 每个上游服务都有一个接口deliver(uid, cmd, bytes), gateway 通过此接口将对应数据包转发给服务

3. 每个上游服务有个app类(单例)，里面有两函数（OnMessage、schedule），deliver调用OnMessage将消息发送到app的channel，channel带有缓冲区（1024或者更大）；schedule里面不断从channel里获取消息来处理

4. 服务架构

5. 使用这种架构上游服务不需要打包、解包Packet，只需要打包、解包具体的业务消息

   ```
                        pub/sub  game.OnMessage(uid, cmd, bytes)
   connector[单节点]  GRPC---------> game
                       
                       sendToPlayer()
   					<-----------
   ```

   




# 第四章 基于http的协议

这种解决方案的有点在于简单



1. 客户端通过http协议请求服务器数据，服务端有一个专门的推送服务，每个客户端和推送服务建立长连接
2. 客户端和服务器采用json格式通信，服务端推送给客户端的数据也是json
3. token的时间可以设置为1个月或者永远







# 第五章 连接服务的设计



## 5.1 采用云风的思路

云风的连接器没有业务，会更加通用

url = https://blog.codingnow.com/2006/04/iocp_kqueue_epoll.html



## 5.2 传统思路

传统思路相对云风的，会更加简洁。

1. client 访问login 返回token，jwt 会把 uid 和 过期时间戳加密放进token
2. connector 收到client登录请求时，解密得出uid，直接和socket 进行绑定





# 第六章 推送服务的设计

跑马灯、公告之类的需求，需要向部分/全部玩家推送数据，因此需要一个单独的推送服务。设计的时候，会把推送服务和session服务合并为一个。

1. client 拿token 连接connector时，会解密出uid和socket绑定，并且将 connectorAddr ：uid 信息注册到session，session如果发现其它地方登录，会调用connector的rpc将原来的用户踢下线

2. 其它请求直接转发到对应的服务，如果发其它请求之前，未发送登录请求，就找不到uid，直接丢包，非常简单

   ```shell
   1、每个链接会有一个localAddress   ip:port
   2、根据localAddress 查找对应uid
   3、如果找到了，转发给特定服务
   4、如果找不到，直接丢包
   ```



# 第六章  财务服务的设计

全部数据库相关的操作，全部封装到财务服务，做成无状态的



# 第七章 登录服务的设计

1. 协议采用http
2. 做成无状态的