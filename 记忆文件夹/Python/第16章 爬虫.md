# 网页数据的采集与urlib库

## 网络库介绍

+ urlib库      http协议常用库，系统标准库

+ request库 http协议常用库，第三方库

+ BeautifulSoup库 xml格式处理库

  ```python
  from urllib import request
  
  url = "http://www.baidu.com"
  response = request.urlopen(url,timeout=1)
  print(response.read().decode("utf-8"))
  ```



## urlib库的使用

### 网页常见的两种请求方式Get与Post

+ 简单介绍Get与Post方法

+ 代码演示

  ```python
  from urllib import parse
  from urllib import request
  #先设置操作系统http、https、ftp代理服务器
  data = bytes(parse.urlencode({"word":"hello"}), encoding="utf8")
  response = request.urlopen("http://httpbin.org/post", data=data, timeout=10)
  print(response.read().decode("utf8"))
  ```
  
  ```python
  import urllib
from urllib import request
  import socket
  #先设置操作系统http、https、ftp代理服务器
  try:
      req = request.urlopen("http://httpbin.org/get?a=888&b=999", timeout=5)
      print(req.read().decode("utf8"))
  except urllib.error.URLError as e:
      if isinstance(e.reason, socket.timeout):
          print("TIME OUT")
  ```

### HTTP头部信息模拟

```python
from urllib import request
from urllib import parse

#构造http头部信息
url = "http://httpbin.org/post"
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, sdch",
    "Accept-Language": "zh-CN,zh;q=0.8",
    "Connection": "close",
    "Cookie": "_gauges_unique_hour=1; _gauges_unique_day=1; _gauges_unique_month=1; _gauges_unique_year=1; _gauges_unique=1",
    "Referer": "http://httpbin.org/",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER"
}

data = bytes(parse.urlencode({"word":"helloHttp8457"}),encoding="utf8")
req = request.Request(url=url,data=data,headers=headers,method="POST")
response = request.urlopen(req,timeout=10)
print(response.read().decode("utf8"))
```



## requests库的基本使用

+ 安装 *pip3 install requests --proxy http://127.0.0.1:8090*

+ 使用第三方包
	+ 直接用项目中的pip安装
	+ 用系统的pip安装，在pycharm中进行设置
		1. 文件-->默认设置-->项目解释器，添加系统解释器
		2. 文件-->设置-->项目解释器  设置python解释器为系统解释器

+ 代码示例
	```python
	import requests
	
	url = "http://httpbin.org/get"
	data = {"key":"value","abc":"xyz"}
	
	response = requests.get(url,data)
	print(response.text)
	print("*" * 20)
	#post 请求
	url = "http://httpbin.org/post"
	data = {"key":"value","abc":"xyz"}
	response = requests.post(url,data)
	print(response.json())
	```
	
+ 结合正则表达式爬取图片链接

```python
import requests
import re

url = 'http://www.cnu.cc/discoveryPage/hot-人像'
content = requests.get(url).text

pattern = re.compile(r'<img src="(.*?)".*?alt="(.*?)">',re.S)
result = re.findall(pattern, content)
for elem in result:
    uri,name = elem
    pos = str(uri).find("?")
    uri = uri[0:pos]
    pattern = re.compile(r".*\.(.*)")
    postfix = re.findall(pattern, uri)[0]
    content = requests.get(uri).content
    name = re.sub("[\"”“ ]","",name)
    try:
        f = open("img/" + name + "." + postfix,"wb")
        f.write(content)
        f.close()
    except Exception as e:
        print(e)
        print("%s.%s 下载失败" %(name,postfix))
print("图片下载完成")
```



# BeautifulSoup

### 安装和基本用法

+ 安装
	1. pip3 install bs4 --proxy http://127.0.0.1:8090
	2. pip3 install lxml --proxy http://127.0.0.1:8090
+ 使用
	1. 基本功能
		+  格式化 *soup.prettify()*
		+  获取标签
		   +  根据标签名获取
		   +  根据id获取
		+  获取标签里面的内容
		+  获取标签属性
		+  获取全部满足指定条件的标签
	2. 示例代码
	```python
	from bs4 import BeautifulSoup
	
	html_doc = """
	<html><head><title>The Dormouse's story</title></head>
	<body>
	<p class="title"><b>The Dormouse's story</b></p>
	
	<p class="story">Once upon a time there were three little sisters; and their names were
	<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
	<a href="http://example.com/lacie" class="
	sister" id="link2">Lacie</a> and
	<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
	and they lived at the bottom of a well.</p>
	
	<p class="story">...</p>
	"""
	
	soup = BeautifulSoup(html_doc, "lxml")
	print(soup.prettify())#格式化
	
	print(soup.title)#获取title标签
	print(soup.title.string)#获取title标签里面的内容
	#找到所有的a标签
	print(soup.find_all("a"))
	print(soup.a["href"])#获取标签的属性
	print(soup.find(id="link3"))#通过id获取标签
	```

### 使用爬虫爬取新闻网站

+ 下载网页源码至html文件里，也可以用***requests.get***获取网页源码
+ 源码
```python
from bs4 import BeautifulSoup

url = "https://www.infoq.cn/"
f = open("src.html","r",encoding="utf-8")
root_path = "https://www.infoq.cn/"
html = f.read()
soup = BeautifulSoup(html,"lxml")
title_hrefs = soup.find_all("div", class_="list-item image-position-right")
for title_href in title_hrefs:
    title = title_href.find("a",class_="com-article-title")
    print("标题：%s 链接:%s" %(title.string, root_path + title.get("href")))
```
### 获取图片链接并下载图片
```python
from bs4 import  BeautifulSoup
import requests
import os
import shutil

#1: 下载图片
#2: 实现翻页
#3: 利用多线程
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.8",
    "Connection": "close",
    "Cookie": "_gauges_unique_hour=1; _gauges_unique_day=1; _gauges_unique_month=1; _gauges_unique_year=1; _gauges_unique=1",
    "Referer": "http://www.infoq.com",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER"
}

def download_img(url,path):
    response = requests.get(url,stream=True)
    if response.status_code == 200:
        with open(path,"wb") as f:
            response.raw.decode_content = True
            shutil.copyfileobj(response.raw, f)
def download_all_img(url):
    response = requests.get(url,headers=headers)
    soup = BeautifulSoup(response.text,"lxml")
    for pic_href in soup.find_all("div", class_="container pc"):
        for pic in pic_href.find_all("img"):
            img_url = pic.get("src")
            pos = str(img_url).find('?')
            img_url = img_url[0:pos]
            # print(img_url)
            dir = os.path.abspath("./img")
            file_name = os.path.basename(img_url)
            img_path = os.path.join(dir,file_name)
            print("开始下载 %s" %img_url)
            download_img(img_url,img_path)


url = "http://www.cnu.cc/discoveryPage/hot-人像?page="
for i in range(1,6):
    print("第 %d 页" %i)
    download_all_img(url + str(i))
```

