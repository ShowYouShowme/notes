# 网页数据的采集与urlib库

## 网络库介绍

+ urlib库      http协议常用库，系统标准库

+ request库 http协议常用库，第三方库

+ BeautifulSoup库 xml格式处理库

  ```python
  from urllib import request
  
  url = "http://www.baidu.com"
  response = request.urlopen(url,timeout=1)
  print(response.read().decode("utf-8"))
  ```



## urlib库的使用

### 网页常见的两种请求方式Get与Post

+ 简单介绍Get与Post方法

+ 代码演示

  ```python
  from urllib import parse
  from urllib import request
  #先设置操作系统http、https、ftp代理服务器
  data = bytes(parse.urlencode({"word":"hello"}), encoding="utf8")
  response = request.urlopen("http://httpbin.org/post", data=data, timeout=10)
  print(response.read().decode("utf8"))
  ```
  
  ```python
  import urllib
from urllib import request
  import socket
  #先设置操作系统http、https、ftp代理服务器
  try:
      req = request.urlopen("http://httpbin.org/get?a=888&b=999", timeout=5)
      print(req.read().decode("utf8"))
  except urllib.error.URLError as e:
      if isinstance(e.reason, socket.timeout):
          print("TIME OUT")
  ```
  
  ```python
  # 用json格式发请求
  from urllib import request
  import json
  
  # 请求体数据
  request_data = {
      "serial": True,
      "items": [{"server_id": 41, "command": "restart"}]
  }
  
  headers = {
      "content-type": "application/json"
  }
  
  req = request.Request(url="http://10.10.10.168:3000/pages/server/api/add_task",
                        headers=headers,
                        data=json.dumps(request_data).encode("utf-8"))
  
  reps = request.urlopen(req).read().decode("utf-8")
  ```
  
  

### HTTP头部信息模拟

```python
from urllib import request
from urllib import parse

#构造http头部信息
url = "http://httpbin.org/post"
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, sdch",
    "Accept-Language": "zh-CN,zh;q=0.8",
    "Connection": "close",
    "Cookie": "_gauges_unique_hour=1; _gauges_unique_day=1; _gauges_unique_month=1; _gauges_unique_year=1; _gauges_unique=1",
    "Referer": "http://httpbin.org/",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER"
}

data = bytes(parse.urlencode({"word":"helloHttp8457"}),encoding="utf8")
req = request.Request(url=url,data=data,headers=headers,method="POST")
response = request.urlopen(req,timeout=10)
print(response.read().decode("utf8"))
```



## requests库的基本使用

+ 安装 *pip3 install requests --proxy http://127.0.0.1:8090*

+ 使用第三方包
	+ 直接用项目中的pip安装
	+ 用系统的pip安装，在pycharm中进行设置
		1. 文件-->默认设置-->项目解释器，添加系统解释器
		2. 文件-->设置-->项目解释器  设置python解释器为系统解释器

+ 代码示例
	```python
	import requests
	
	url = "http://httpbin.org/get"
	data = {"key":"value","abc":"xyz"}
	
	response = requests.get(url,data)
	print(response.text)
	print("*" * 20)
	#post 请求
	url = "http://httpbin.org/post"
	data = {"key":"value","abc":"xyz"}
	response = requests.post(url,data)
	print(response.json())
	```
	
+ 结合正则表达式爬取图片链接

```python
import requests
import re

url = 'http://www.cnu.cc/discoveryPage/hot-人像'
content = requests.get(url).text

pattern = re.compile(r'<img src="(.*?)".*?alt="(.*?)">',re.S)
result = re.findall(pattern, content)
for elem in result:
    uri,name = elem
    pos = str(uri).find("?")
    uri = uri[0:pos]
    pattern = re.compile(r".*\.(.*)")
    postfix = re.findall(pattern, uri)[0]
    content = requests.get(uri).content
    name = re.sub("[\"”“ ]","",name)
    try:
        f = open("img/" + name + "." + postfix,"wb")
        f.write(content)
        f.close()
    except Exception as e:
        print(e)
        print("%s.%s 下载失败" %(name,postfix))
print("图片下载完成")
```



# BeautifulSoup

### 安装和基本用法

+ 安装
	1. pip3 install bs4 --proxy http://127.0.0.1:8090
	2. pip3 install lxml --proxy http://127.0.0.1:8090
+ 使用
	1. 基本功能
		+  格式化 *soup.prettify()*
		+  获取标签
		   +  根据标签名获取
		   +  根据id获取
		+  获取标签里面的内容
		+  获取标签属性
		+  获取全部满足指定条件的标签
	2. 示例代码
	```python
	from bs4 import BeautifulSoup
	
	html_doc = """
	<html><head><title>The Dormouse's story</title></head>
	<body>
	<p class="title"><b>The Dormouse's story</b></p>
	
	<p class="story">Once upon a time there were three little sisters; and their names were
	<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
	<a href="http://example.com/lacie" class="
	sister" id="link2">Lacie</a> and
	<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
	and they lived at the bottom of a well.</p>
	
	<p class="story">...</p>
	"""
	
	soup = BeautifulSoup(html_doc, "lxml")
	print(soup.prettify())#格式化
	
	print(soup.title)#获取title标签
	print(soup.title.string)#获取title标签里面的内容
	#找到所有的a标签
	print(soup.find_all("a"))
	print(soup.a["href"])#获取标签的属性
	print(soup.find(id="link3"))#通过id获取标签
	```

### 使用爬虫爬取新闻网站

+ 下载网页源码至html文件里，也可以用***requests.get***获取网页源码
+ 源码
```python
from bs4 import BeautifulSoup

url = "https://www.infoq.cn/"
f = open("src.html","r",encoding="utf-8")
root_path = "https://www.infoq.cn/"
html = f.read()
soup = BeautifulSoup(html,"lxml")
title_hrefs = soup.find_all("div", class_="list-item image-position-right")
for title_href in title_hrefs:
    title = title_href.find("a",class_="com-article-title")
    print("标题：%s 链接:%s" %(title.string, root_path + title.get("href")))
```
### 获取图片链接并下载图片
```python
from bs4 import  BeautifulSoup
import requests
import os
import shutil

#1: 下载图片
#2: 实现翻页
#3: 利用多线程
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.8",
    "Connection": "close",
    "Cookie": "_gauges_unique_hour=1; _gauges_unique_day=1; _gauges_unique_month=1; _gauges_unique_year=1; _gauges_unique=1",
    "Referer": "http://www.infoq.com",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER"
}

def download_img(url,path):
    response = requests.get(url,stream=True)
    if response.status_code == 200:
        with open(path,"wb") as f:
            response.raw.decode_content = True
            shutil.copyfileobj(response.raw, f)
def download_all_img(url):
    response = requests.get(url,headers=headers)
    soup = BeautifulSoup(response.text,"lxml")
    for pic_href in soup.find_all("div", class_="container pc"):
        for pic in pic_href.find_all("img"):
            img_url = pic.get("src")
            pos = str(img_url).find('?')
            img_url = img_url[0:pos]
            # print(img_url)
            dir = os.path.abspath("./img")
            file_name = os.path.basename(img_url)
            img_path = os.path.join(dir,file_name)
            print("开始下载 %s" %img_url)
            download_img(img_url,img_path)


url = "http://www.cnu.cc/discoveryPage/hot-人像?page="
for i in range(1,6):
    print("第 %d 页" %i)
    download_all_img(url + str(i))
```







# 文件上传

1. http头部

   ```shell
   Content-Type: multipart/form-data; boundary=${bound}
   ```

2. 示例代码

   ```python
   import os, random, sys, requests
   from requests_toolbelt.multipart.encoder import MultipartEncoder
   
   url = 'http://10.10.10.23:8081/file_upload'
   headers = {
       'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0',
       'Referer': url
       }
   
   file = './cros.html'
   # image 是表单的字段名
   multipart_encoder = MultipartEncoder(
       fields={
           'image': (os.path.basename(file) , open(file, 'rb'), 'application/octet-stream')
           #file为路径
           },
           boundary='-----------------------------' + str(random.randint(1e28, 1e29 - 1))
       )
   
   headers['Content-Type'] = multipart_encoder.content_type
   #请求头必须包含一个特殊的头信息，类似于Content-Type: multipart/form-data; boundary=${bound}
   
   r = requests.post(url, data=multipart_encoder, headers=headers)
   print(r.text)
   ```

   ```html
   <!DOCTYPE html>
   <html lang="en">
   <head>
       <meta charset="UTF-8">
       <title>文件上传演示</title>
   </head>
   <body>
   <h3>文件上传:</h3>
   选择一个文件上传:<br/>
   <form action="file_upload" method="post" enctype="multipart/form-data">
       <input type="file" name="image" size="50">
       <br/>
       <input type="submit" value="文件上传">
   </form>
   </body>
   </html>
   ```

   ```javascript
   import express = require("express");
   import fs = require("fs");
   import bodyParser = require("body-parser");
   import multer = require("multer");
   
   
   
   let app : express.Application = express();
   app.use(express.static('public'));
   app.use(bodyParser.urlencoded({extended : false}));
   app.use(multer({dest:'/tmp'}).array('image'));
   
   app.get("/", (req: express.Request, res : express.Response):void=>{
       let path : string = __dirname + "/" +"form.html"
       res.sendFile(path);
   });
   
   app.post('/file_upload', (req : express.Request, res : express.Response):void=>{
       let files   : Express.Multer.File[] = req.files as Express.Multer.File[];
       let file    : Express.Multer.File   = files[0];
       console.log(file);
   
       let des_file : string = __dirname + "/" + file.originalname;
       fs.readFile(file.path, (err, data):void=>{
           fs.writeFile(des_file, data, (err):void=>{
               let response : any = null;
               if (err){
                   console.error(err);
               } else{
                   response = {
                       message:'File uploaded successfully',
                       filename:file.originalname
                   }
               }
               console.log(response);
               res.end(JSON.stringify(response));
           })
       })
   
   });
   
   let server = app.listen(8081);
   ```

   